{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/waltersbrf/Doutorado/blob/main/FAQ_com_Langchain_e_LLMs_OpenAi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Objetivo do Notebook "
      ],
      "metadata": {
        "id": "INIvU1T_b4Zx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introdução"
      ],
      "metadata": {
        "id": "0t_aD4VMGJrm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LangChain é um framework para desenvolvimento de aplicações usando Grandes Modelos de Linguagem (Large Language Model - LLMs). Com a combinação  das LLMs com outros ativos é possível gerar aplicações que envolvem gerenciamento de prompt, encadeamento (chain), geração de dados aumentados(data augmentation), designação de sequência de operações, etc.\n",
        "\n",
        "\n",
        "No caso deste notebook, será apresentado um processo de criação de FAQ (Frequently Asked Questions Em português seria algo como \"Perguntas frequentes\") sobre um tema. No caso, o tema em questão envolve atores de um sistema de rastreio de satélites.\n",
        "\n",
        "\n",
        "Para trabalhar num tema com LangChain, será necessário informar alguns ativos para servir de base para o processamento, no caso esse exemplo irá fornecer um conjunto de manuais e artigos sobre o tema. \n",
        "\n",
        "Como observação, as respostas devem variar de acordo com o material de suporte fornecido. \n",
        "\n",
        "Os esforços destes notebook focam na execução deste conjunto no Google colab."
      ],
      "metadata": {
        "id": "6DbbWOzsGNAe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBB4ZzBfaxqa",
        "outputId": "cffc4e70-4021-4ba9-a85e-d1f6f97944bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"seu token aqui - OpenIA\""
      ],
      "metadata": {
        "id": "BxfSXueejugW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "pdf_folder_path = '/content/gdrive/MyDrive/data2'"
      ],
      "metadata": {
        "id": "nG5Op0HPkIyN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instalando dependências "
      ],
      "metadata": {
        "id": "VGCiSh6YRxTO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5x9SQkWLRrGw"
      },
      "outputs": [],
      "source": [
        "!pip install openai\n",
        "\n",
        "!pip install langchain\n",
        "!pip install sentence_transformers\n",
        "!pip install faiss-cpu\n",
        "\n",
        "!pip install unstructured\n",
        "!pip install chromadb\n",
        "!pip install Cython\n",
        "!pip install tiktoken\n",
        "!pip install unstructured[local-inference]\n",
        "\n",
        "!pip install pdf2image\n",
        "!pip install pytesseract\n",
        "\n",
        "!pip install PyPDF2\n",
        "\n",
        "!apt-get install poppler-utils tesseract-ocr"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementando as Operações"
      ],
      "metadata": {
        "id": "g2sR9KMtcH3J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os \n",
        "\n",
        "from langchain.document_loaders import UnstructuredPDFLoader\n",
        "from langchain.indexes import VectorstoreIndexCreator\n",
        "\n",
        "\n",
        "loaders = [UnstructuredPDFLoader(os.path.join(pdf_folder_path, fn)) for fn in os.listdir(pdf_folder_path)]\n",
        "loaders"
      ],
      "metadata": {
        "id": "gP-7oM0ZK3tU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54b902bd-bd57-48be-be6b-2a45dd4b494b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<langchain.document_loaders.pdf.UnstructuredPDFLoader at 0x7f5c965735e0>]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from PyPDF2 import PdfReader\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "#from langchain.vectorstores import ElasticVectorSearch, Pinecone, Weaviate, FAISS\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import RetrievalQA"
      ],
      "metadata": {
        "id": "Ak8RmNJISEqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = VectorstoreIndexCreator(\n",
        "    embedding= OpenAIEmbeddings(),\n",
        "    text_splitter=CharacterTextSplitter(chunk_size=2500, chunk_overlap=0)).from_loaders(loaders)"
      ],
      "metadata": {
        "id": "OLOc4nPdLQ6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "chain = RetrievalQA.from_chain_type(llm=OpenAI() ,\n",
        "                                    chain_type=\"stuff\", \n",
        "                                    retriever=index.vectorstore.as_retriever(), \n",
        "                                    input_key=\"question\")\n"
      ],
      "metadata": {
        "id": "iCHVR31kPrPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.run('Os textos falam sobre o que?')"
      ],
      "metadata": {
        "id": "I8V5wXxKjV6y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}